{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbowEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, winlen):\n",
    "        super().__init__()\n",
    "        self.embedding_weights = nn.Parameter(torch.randn(vocab_size, embedding_dim, dtype = torch.float32, device = device) * (1.0 / embedding_dim ** 0.5))\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_vecs = self.embedding_weights[context] # batch_size * (winlen - 1) * embedding\n",
    "        avg_vecs = context_vecs.mean(dim = 1)\n",
    "        output = self.fc(avg_vecs)\n",
    "        return output\n",
    "    \n",
    "    def getEmbedding(self, id):\n",
    "        return self.embedding_weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading wiki-103.train.tokens: 100%|██████████| 541M/541M [08:32<00:00, 1.06MB/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m     46\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines)\n\u001b[0;32m---> 47\u001b[0m words, word2id, id2word \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m context_train, center_train, vocab_size, word_size \u001b[38;5;241m=\u001b[39m generateData(words, word2id, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(context_train\u001b[38;5;241m.\u001b[39mshape, center_train\u001b[38;5;241m.\u001b[39mshape, vocab_size)\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mpreprocessing\u001b[0;34m(text, min_count, threshold)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     11\u001b[0m     freq \u001b[38;5;241m=\u001b[39m word_counts[word] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(words)\n\u001b[0;32m---> 12\u001b[0m     discard_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m>\u001b[39m discard_prob:\n\u001b[1;32m     15\u001b[0m         sub_words\u001b[38;5;241m.\u001b[39mappend(word)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocessing(text, min_count = 15, threshold = 1e-5):\n",
    "    _ = re.findall(r\"[A-Za-z]+\", text)\n",
    "    words = []\n",
    "    for word in _:\n",
    "        words.append(word.lower())\n",
    "    word_counts = Counter(words)\n",
    "    words = [word for word in words if word_counts[word] >= min_count]\n",
    "\n",
    "    sub_words = []\n",
    "    for word in words:\n",
    "        freq = word_counts[word] / len(words)\n",
    "        discard_prob = 1.0 - np.sqrt(threshold / freq)\n",
    "\n",
    "        if np.random.rand() > discard_prob:\n",
    "            sub_words.append(word)\n",
    "\n",
    "    word2id = {w : i for i, w in enumerate(set(sub_words))}\n",
    "    id2word = {i : w for _, (w, i) in enumerate(word2id.items())}\n",
    "    return words, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen): # winlen must be odd\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    context_train = np.zeros((batch_size, winlen - 1))\n",
    "    center_train = np.zeros((batch_size))\n",
    "    for _ in range(winlen // 2, word_size - winlen // 2):\n",
    "        fr = _ - winlen // 2\n",
    "        center_train[fr] = word2id[words[_]]\n",
    "        for __ in range(_ - winlen // 2, _):\n",
    "            context_train[fr][__ - (_ - winlen // 2)] = word2id[words[__]]\n",
    "        for __ in range(_ + 1, _ + winlen // 2 + 1):\n",
    "            context_train[fr][__ - (_ - winlen // 2) - 1] = word2id[words[__]]\n",
    "    return torch.tensor(context_train).int(), torch.tensor(center_train).int(), vocab_size, word_size\n",
    "\n",
    "file_path = \"wiki-103.train.tokens\"\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, vocab_size, word_size = generateData(words, word2id, 7)\n",
    "print(context_train.shape, center_train.shape, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, context_train, center_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_epoches = 100\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_context, batch_center in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epoches}', leave=False):\n",
    "            # Move data to device\n",
    "            batch_context = batch_context.to(device)\n",
    "            batch_center = batch_center.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_context)\n",
    "            target = batch_center.long()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * len(batch_context)\n",
    "        \n",
    "        epoch_loss /= len(context_train)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "        elif epoch > 20 and loss > best_loss * 1.05: \n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = CbowEmbedding(vocab_size, 256, 7).to(device)\n",
    "model.embedding_weights.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_model = torch.load(\"model-103.pth\", map_location=device)\n",
    "#model.load_state_dict(current_model[\"state_dict\"])\n",
    "#optimizer.load_state_dict(current_model[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset = CorpusDataset(context_train, center_train), \n",
    "    batch_size = 4096, \n",
    "    shuffle = True, \n",
    "    num_workers = 4,\n",
    "    pin_memory = True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(len(dataloader))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} | Device: {param.device}\")\n",
    "model, optimizer = train(model, optimizer, context_train, center_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach().to(\"cpu\")\n",
    "    embedding = embedding / torch.norm(embedding)\n",
    "    word2embed[word] = embedding\n",
    "\n",
    "with open(\"output-cbow.txt\", 'w') as f:\n",
    "    for (word, embed) in word2embed.items():\n",
    "        f.write(word)\n",
    "        f.write(str(list(embed.numpy())))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClose(target, k = 5):\n",
    "    sims = []\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    for (word, embed) in word2embed.items():\n",
    "        sim = cos(embed, target).item()\n",
    "        sims.append((word, sim))\n",
    "\n",
    "    res = sorted(sims, key=lambda x : x[1], reverse = True)\n",
    "    return [_[0] for _ in res[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "correctCount = 0\n",
    "totalCount = 0\n",
    "with open(\"questions-words.txt\", 'r') as f:\n",
    "    qs = f.read()\n",
    "\n",
    "qs_s = qs.split('\\n')\n",
    "random.shuffle(qs_s)\n",
    "\n",
    "for q in qs_s[:100]:\n",
    "    words = q.split()\n",
    "    try:\n",
    "        target = word2embed[words[2].lower()] + word2embed[words[1].lower()] - word2embed[words[0].lower()]\n",
    "        ansembed = word2embed[words[3].lower()]\n",
    "        target = target / torch.norm(target)\n",
    "        ans = getClose(target, 10)\n",
    "        if words[3].lower() in ans:\n",
    "            correctCount += 1\n",
    "        print(words[0].lower(), words[1].lower(), words[2].lower(), ans)\n",
    "        totalCount += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(correctCount, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'state_dict': model.state_dict(),  # model parameters\n",
    "    'optimizer': optimizer.state_dict(),  # optimizer state\n",
    "}\n",
    "torch.save(state, 'model-103.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = WordCompleter(list(word2embed.keys()))\n",
    "\n",
    "while True:\n",
    "    word = prompt(\"Word: \", completer = completer)\n",
    "    cc = int(input(\"Input closest count: \"))\n",
    "    try:\n",
    "        embed = word2embed[word]\n",
    "        print(embed, *getClose(embed, cc))\n",
    "    except KeyError:\n",
    "        print(\"Nonexistent word.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

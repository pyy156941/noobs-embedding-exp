{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbowEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, winlen):\n",
    "        super().__init__()\n",
    "        self.embedding_weights = nn.Parameter(torch.randn(vocab_size, embedding_dim, dtype = torch.float32, device = device) * (1.0 / embedding_dim ** 0.5))\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_vecs = self.embedding_weights[context] # batch_size * (winlen - 1) * embedding\n",
    "        avg_vecs = context_vecs.mean(dim = 1)\n",
    "        output = self.fc(avg_vecs)\n",
    "        return output\n",
    "    \n",
    "    def getEmbedding(self, id):\n",
    "        return self.embedding_weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, min_count = 15, threshold = 1e-5):\n",
    "    _ = re.findall(r\"[A-Za-z]+\", text)\n",
    "    words = []\n",
    "    for word in _:\n",
    "        words.append(word.lower())\n",
    "    word_counts = Counter(words)\n",
    "    words = [word for word in words if word_counts[word] >= min_count]\n",
    "\n",
    "    sub_words = []\n",
    "    for word in words:\n",
    "        freq = word_counts[word] / len(words)\n",
    "        discard_prob = 1.0 - np.sqrt(threshold / freq)\n",
    "\n",
    "        if np.random.rand() > discard_prob:\n",
    "            sub_words.append(word)\n",
    "\n",
    "    word2id = {w : i for i, w in enumerate(set(sub_words))}\n",
    "    id2word = {i : w for _, (w, i) in enumerate(word2id.items())}\n",
    "    return words, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen): # winlen must be odd\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    context_train = np.zeros((batch_size, winlen - 1))\n",
    "    center_train = np.zeros((batch_size))\n",
    "    for _ in range(winlen // 2, word_size - winlen // 2):\n",
    "        fr = _ - winlen // 2\n",
    "        center_train[fr] = word2id[words[_]]\n",
    "        for __ in range(_ - winlen // 2, _):\n",
    "            context_train[fr][__ - (_ - winlen // 2)] = word2id[words[__]]\n",
    "        for __ in range(_ + 1, _ + winlen // 2 + 1):\n",
    "            context_train[fr][__ - (_ - winlen // 2) - 1] = word2id[words[__]]\n",
    "    return torch.tensor(context_train).int(), torch.tensor(center_train).int(), vocab_size, word_size\n",
    "\n",
    "file_path = \"wiki-103.train.tokens\"\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, vocab_size, word_size = generateData(words, word2id, 7)\n",
    "print(context_train.shape, center_train.shape, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, context_train, center_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_epoches = 100\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_context, batch_center in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epoches}', leave=False):\n",
    "            # Move data to device\n",
    "            batch_context = batch_context.to(device)\n",
    "            batch_center = batch_center.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_context)\n",
    "            target = batch_center.long()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * len(batch_context)\n",
    "        \n",
    "        epoch_loss /= len(context_train)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "        elif epoch > 20 and loss > best_loss * 1.05: \n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = CbowEmbedding(vocab_size, 256, 7).to(device)\n",
    "model.embedding_weights.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_model = torch.load(\"model-103.pth\", map_location=device)\n",
    "#model.load_state_dict(current_model[\"state_dict\"])\n",
    "#optimizer.load_state_dict(current_model[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset = CorpusDataset(context_train, center_train), \n",
    "    batch_size = 4096, \n",
    "    shuffle = True, \n",
    "    num_workers = 4,\n",
    "    pin_memory = True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(len(dataloader))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} | Device: {param.device}\")\n",
    "model, optimizer = train(model, optimizer, context_train, center_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach().to(\"cpu\")\n",
    "    embedding = embedding / torch.norm(embedding)\n",
    "    word2embed[word] = embedding\n",
    "\n",
    "with open(\"output-cbow.txt\", 'w') as f:\n",
    "    for (word, embed) in word2embed.items():\n",
    "        f.write(word)\n",
    "        f.write(str(list(embed.numpy())))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClose(target, k = 5):\n",
    "    sims = []\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    for (word, embed) in word2embed.items():\n",
    "        sim = cos(embed, target).item()\n",
    "        sims.append((word, sim))\n",
    "\n",
    "    res = sorted(sims, key=lambda x : x[1], reverse = True)\n",
    "    return [_[0] for _ in res[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "correctCount = 0\n",
    "totalCount = 0\n",
    "with open(\"questions-words.txt\", 'r') as f:\n",
    "    qs = f.read()\n",
    "\n",
    "qs_s = qs.split('\\n')\n",
    "random.shuffle(qs_s)\n",
    "\n",
    "for q in qs_s[:100]:\n",
    "    words = q.split()\n",
    "    try:\n",
    "        target = word2embed[words[2].lower()] + word2embed[words[1].lower()] - word2embed[words[0].lower()]\n",
    "        ansembed = word2embed[words[3].lower()]\n",
    "        target = target / torch.norm(target)\n",
    "        ans = getClose(target, 10)\n",
    "        if words[3].lower() in ans:\n",
    "            correctCount += 1\n",
    "        print(words[0].lower(), words[1].lower(), words[2].lower(), ans)\n",
    "        totalCount += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(correctCount, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'state_dict': model.state_dict(),  # model parameters\n",
    "    'optimizer': optimizer.state_dict(),  # optimizer state\n",
    "}\n",
    "torch.save(state, 'model-103.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = WordCompleter(list(word2embed.keys()))\n",
    "\n",
    "while True:\n",
    "    word = prompt(\"Word: \", completer = completer)\n",
    "    cc = int(input(\"Input closest count: \"))\n",
    "    try:\n",
    "        embed = word2embed[word]\n",
    "        print(embed, *getClose(embed, cc))\n",
    "    except KeyError:\n",
    "        print(\"Nonexistent word.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

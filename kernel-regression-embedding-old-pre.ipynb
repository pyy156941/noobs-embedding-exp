{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
    "nlp.max_length = 100_000_0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KREmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.embedding_weights = nn.Parameter(torch.randn(vocab_size, embedding_dim, dtype = torch.float32, device = device) * (1.0 / embedding_dim ** 0.5))\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, context, center):\n",
    "        context_vecs = self.embedding_weights[context] # batch_size * (winlen - 1) * embedding\n",
    "        center_vec = self.embedding_weights[center] # batch_size * embedding\n",
    "        diff = context_vecs - center_vec.unsqueeze(1)  # batch_size * (winlen - 1) * embedding\n",
    "        dist_sq = torch.sum(diff ** 2, dim=2)  # batch_size * (winlen - 1)\n",
    "        weights = torch.exp(-dist_sq / (2 * self.sigma ** 2))  # batch_size * (winlen - 1)\n",
    "        weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-8)  # batch_size * (winlen - 1)\n",
    "        weighted_context = (weights.unsqueeze(2) * context_vecs).sum(dim=1)  # batch_size * embedding\n",
    "\n",
    "        similarity_matrix = torch.mm(weighted_context, self.embedding_weights.t())\n",
    "        return similarity_matrix # compare to embeddings and output logits\n",
    "    \n",
    "    def getEmbedding(self, id):\n",
    "        return self.embedding_weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([82586969, 6]) torch.Size([82586969]) 89807\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text, min_count=5, threshold=1e-5, chunk_size=500000):\n",
    "    # Stage 1: Fast Gensim cleaning (memory-safe)\n",
    "    words = simple_preprocess(text, deacc=True, min_len=2)\n",
    "    \n",
    "    # Stage 2: Chunked SpaCy lemmatization\n",
    "    def process_chunk(chunk):\n",
    "        doc = nlp(\" \".join(chunk))\n",
    "        return [\n",
    "            token.lemma_ for token in doc\n",
    "            if token.is_alpha and not token.is_punct and len(token) > 1\n",
    "        ]\n",
    "    \n",
    "    # Split words into chunks to avoid SpaCy memory issues\n",
    "    lemmatized = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = words[i : i + chunk_size]\n",
    "        lemmatized.extend(process_chunk(chunk))\n",
    "    \n",
    "    # Stage 3: Subsampling (Word2Vec style)\n",
    "    word_counts = Counter(lemmatized)\n",
    "    total_words = len(lemmatized)\n",
    "    subsampled = [\n",
    "        word for word in lemmatized\n",
    "        if word_counts[word] >= min_count and \n",
    "            np.random.rand() > (1 - np.sqrt(threshold / (word_counts[word]/total_words)))\n",
    "    ]\n",
    "    \n",
    "    # Build vocab\n",
    "    vocab = list(set(subsampled))\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    id2word = {i: w for i, w in enumerate(vocab)}\n",
    "    \n",
    "    return subsampled, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen): # winlen must be odd\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    context_train = np.zeros((batch_size, winlen - 1))\n",
    "    center_train = np.zeros((batch_size))\n",
    "    for _ in range(winlen // 2, word_size - winlen // 2):\n",
    "        fr = _ - winlen // 2\n",
    "        center_train[fr] = word2id[words[_]]\n",
    "        for __ in range(_ - winlen // 2, _):\n",
    "            context_train[fr][__ - (_ - winlen // 2)] = word2id[words[__]]\n",
    "        for __ in range(_ + 1, _ + winlen // 2 + 1):\n",
    "            context_train[fr][__ - (_ - winlen // 2) - 1] = word2id[words[__]]\n",
    "    return torch.tensor(context_train).int(), torch.tensor(center_train).int(), vocab_size, word_size\n",
    "        \n",
    "with open(\"wiki-103.train.tokens\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, vocab_size, word_size = generateData(words, word2id, 7)\n",
    "print(context_train.shape, center_train.shape, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_epoches = 100\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_context, batch_center in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epoches}'):\n",
    "            # Move data to device\n",
    "            batch_context = batch_context.to(device)\n",
    "            batch_center = batch_center.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_context, batch_center)\n",
    "            target = batch_center.long()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * len(batch_context)\n",
    "        \n",
    "        epoch_loss /= len(context_train)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "        elif epoch > 20 and loss > best_loss * 1.05: \n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = KREmbedding(vocab_size, 256, 7).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset = CorpusDataset(context_train, center_train), \n",
    "    batch_size = 4096, \n",
    "    shuffle = True, \n",
    "    num_workers = 4,\n",
    "    pin_memory = True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(len(dataloader))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} | Device: {param.device}\")\n",
    "model, optimizer = train(model, optimizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach().to(\"cpu\")\n",
    "    embedding = embedding / torch.norm(embedding)\n",
    "    word2embed[word] = embedding\n",
    "\n",
    "with open(\"output-cbow.txt\", 'w') as f:\n",
    "    for (word, embed) in word2embed.items():\n",
    "        f.write(word)\n",
    "        f.write(str(list(embed.numpy())))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClose(target, k = 5):\n",
    "    sims = []\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    for (word, embed) in word2embed.items():\n",
    "        sim = cos(embed, target).item()\n",
    "        sims.append((word, sim))\n",
    "\n",
    "    res = sorted(sims, key=lambda x : x[1], reverse = True)\n",
    "    return [_[0] for _ in res[:k]]\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "correctCount = 0\n",
    "totalCount = 0\n",
    "with open(\"questions-words.txt\", 'r') as f:\n",
    "    qs = f.read()\n",
    "\n",
    "qs_s = qs.split('\\n')\n",
    "random.shuffle(qs_s)\n",
    "\n",
    "for q in qs_s[:100]:\n",
    "    words = q.split()\n",
    "    try:\n",
    "        target = word2embed[words[2].lower()] + word2embed[words[1].lower()] - word2embed[words[0].lower()]\n",
    "        target = target / torch.norm(target)\n",
    "        ans = getClose(target, 10)\n",
    "        if words[3].lower() in ans:\n",
    "            correctCount += 1\n",
    "        print(words[0].lower(), words[1].lower(), words[2].lower(), ans)\n",
    "        totalCount += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(correctCount, totalCount)\n",
    "\n",
    "state = {\n",
    "    \"state_dict\" : model.state_dict(),  # model parameters\n",
    "    \"optimizer\" : optimizer.state_dict(),  # optimizer state\n",
    "    \"word2id\" : word2id, \n",
    "    \"id2word\" : id2word\n",
    "}\n",
    "torch.save(state, 'model-kop103.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = WordCompleter(list(word2embed.keys()))\n",
    "\n",
    "while True:\n",
    "    word = prompt(\"Word: \", completer = completer)\n",
    "    cc = int(input(\"Input closest count: \"))\n",
    "    try:\n",
    "        embed = word2embed[word]\n",
    "        print(embed, *getClose(embed, cc))\n",
    "    except KeyError:\n",
    "        print(\"Nonexistent word.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
    "nlp.max_length = 100_000_0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KREmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sigma=1.0, num_negatives=5):\n",
    "        super().__init__()\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        nn.init.normal_(self.center_embeddings.weight, mean=0, std=1 / (embedding_dim ** 2))\n",
    "        nn.init.normal_(self.context_embeddings.weight, mean=0, std=1 / (embedding_dim ** 2))\n",
    "        \n",
    "        self.sigma = sigma\n",
    "        self.num_negatives = num_negatives\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, context, center, neg_samples=None):\n",
    "        context_vecs = self.center_embeddings(context)  # [batch_size, winlen-1, embedding_dim]\n",
    "        center_vec = self.context_embeddings(center)  # [batch_size, embedding_dim]\n",
    "        neg_vecs = self.context_embeddings(neg_samples)  # [batch_size, num_negatives, embedding_dim]\n",
    "        diff = context_vecs - center_vec.unsqueeze(1)  # [batch_size, winlen-1, embedding_dim]\n",
    "        dist_sq = torch.sum(diff**2, dim=2)  # [batch_size, winlen-1]\n",
    "        weights = torch.exp(-dist_sq / (2 * self.sigma**2))  # [batch_size, winlen-1]\n",
    "        weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-8)  # Normalize\n",
    "        weighted_context = (weights.unsqueeze(2) * context_vecs).sum(dim=1)  # [batch_size, embedding_dim]\n",
    "        return weighted_context, center_vec, neg_vecs\n",
    "\n",
    "    def getEmbedding(self, word_id):\n",
    "        word_tensor = torch.tensor(word_id, device=self.center_embeddings.weight.device)\n",
    "        return self.center_embeddings(word_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, contexts, centers, neg_samples):\n",
    "        self.contexts = contexts\n",
    "        self.centers = centers\n",
    "        self.neg_samples = neg_samples  # New: store negative samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.centers[idx], self.neg_samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, min_count=5, threshold=1e-5, chunk_size=500000):\n",
    "    # Stage 1: Fast Gensim cleaning (memory-safe)\n",
    "    words = simple_preprocess(text, deacc=True, min_len=2)\n",
    "    \n",
    "    # Stage 2: Chunked SpaCy lemmatization\n",
    "    def process_chunk(chunk):\n",
    "        doc = nlp(\" \".join(chunk))\n",
    "        return [\n",
    "            token.lemma_ for token in doc\n",
    "            if token.is_alpha and not token.is_punct and len(token) > 1\n",
    "        ]\n",
    "    \n",
    "    # Split words into chunks to avoid SpaCy memory issues\n",
    "    lemmatized = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = words[i : i + chunk_size]\n",
    "        lemmatized.extend(process_chunk(chunk))\n",
    "    \n",
    "    # Stage 3: Subsampling (Word2Vec style)\n",
    "    word_counts = Counter(lemmatized)\n",
    "    total_words = len(lemmatized)\n",
    "    subsampled = [\n",
    "        word for word in lemmatized\n",
    "        if word_counts[word] >= min_count and \n",
    "            np.random.rand() > (1 - np.sqrt(threshold / (word_counts[word]/total_words)))\n",
    "    ]\n",
    "    \n",
    "    # Build vocab\n",
    "    vocab = list(set(subsampled))\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    id2word = {i: w for i, w in enumerate(vocab)}\n",
    "    \n",
    "    return subsampled, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen, num_negatives=15):\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    word_counts = Counter(words)\n",
    "    word_freq = np.array([word_counts[id2word[i]] for i in range(vocab_size)])\n",
    "    noise_dist = word_freq ** 0.75\n",
    "    noise_dist /= noise_dist.sum()\n",
    "    \n",
    "    # Pre-allocate numpy arrays\n",
    "    context_train = np.zeros((batch_size, winlen - 1), dtype=np.int32)\n",
    "    center_train = np.zeros((batch_size), dtype=np.int32)\n",
    "    neg_samples = np.zeros((batch_size, num_negatives), dtype=np.int32)\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    neg_samples = rng.choice(\n",
    "        vocab_size, \n",
    "        size=(batch_size, num_negatives), \n",
    "        replace=True, \n",
    "        p=noise_dist\n",
    "    )\n",
    "    for i in range(winlen // 2, len(words) - winlen // 2):\n",
    "        idx = i - winlen // 2\n",
    "        \n",
    "        context = words[i - winlen // 2 : i] + words[i + 1 : i + winlen // 2 + 1]\n",
    "        context_train[idx] = [word2id[w] for w in context]\n",
    "        center_train[idx] = word2id[words[i]]\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(context_train),\n",
    "        torch.tensor(center_train),\n",
    "        torch.tensor(neg_samples),\n",
    "        vocab_size,\n",
    "        len(words)\n",
    "    )\n",
    "        \n",
    "with open(\"wiki-103.train.tokens\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, neg_train, vocab_size, word_size = generateData(words, word2id, 7)\n",
    "print(context_train.shape, center_train.shape, neg_train.shape, vocab_size, context_train.dtype)\n",
    "\n",
    "# with open(\"words.txt\", 'w') as f:\n",
    "#     f.write(str(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, weighted_context, center_vec, neg_vecs):\n",
    "        # Positive score (kernel-weighted context vs center)\n",
    "        pos_score = torch.sum(weighted_context * center_vec, dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "        \n",
    "        # Negative scores (kernel-weighted context vs negative samples)\n",
    "        neg_scores = torch.bmm(neg_vecs, weighted_context.unsqueeze(2)).squeeze()\n",
    "        neg_loss = F.logsigmoid(-neg_scores).sum(dim=1)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    criterion = NegativeSamplingLoss() if hasattr(model, 'num_negatives') else nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}'):\n",
    "            batch_context, batch_center, batch_negs = batch\n",
    "            batch_context, batch_center, batch_negs = batch_context.to(device), batch_center.to(device), batch_negs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            context_vec, center_vec, neg_vecs = model(batch_context, batch_center, batch_negs)\n",
    "            loss = criterion(context_vec, center_vec, neg_vecs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss/len(dataloader)}\")\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = KREmbedding(vocab_size, 256, 7).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset = CorpusDataset(context_train, center_train, neg_train), \n",
    "    batch_size = 131072, \n",
    "    shuffle = True, \n",
    "    num_workers = 4,\n",
    "    pin_memory = True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(len(dataloader))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} | Device: {param.device}\")\n",
    "model, optimizer = train(model, optimizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach().to(\"cpu\")\n",
    "    embedding = embedding / torch.norm(embedding)\n",
    "    word2embed[word] = embedding\n",
    "\n",
    "with open(\"output-cbow.txt\", 'w') as f:\n",
    "    for (word, embed) in word2embed.items():\n",
    "        f.write(word)\n",
    "        f.write(str(list(embed.numpy())))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClose(target, k = 5):\n",
    "    sims = []\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    for (word, embed) in word2embed.items():\n",
    "        sim = cos(embed, target).item()\n",
    "        sims.append((word, sim))\n",
    "\n",
    "    res = sorted(sims, key=lambda x : x[1], reverse = True)\n",
    "    return [_[0] for _ in res[:k]]\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "correctCount = 0\n",
    "totalCount = 0\n",
    "with open(\"questions-words.txt\", 'r') as f:\n",
    "    qs = f.read()\n",
    "\n",
    "qs_s = qs.split('\\n')\n",
    "random.shuffle(qs_s)\n",
    "\n",
    "for q in qs_s[:100]:\n",
    "    words = q.split()\n",
    "    try:\n",
    "        target = word2embed[words[2].lower()] + word2embed[words[1].lower()] - word2embed[words[0].lower()]\n",
    "        target = target / torch.norm(target)\n",
    "        ans = getClose(target, 10)\n",
    "        if words[3].lower() in ans:\n",
    "            correctCount += 1\n",
    "        print(words[0].lower(), words[1].lower(), words[2].lower(), ans)\n",
    "        totalCount += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(correctCount, totalCount)\n",
    "\n",
    "state = {\n",
    "    \"state_dict\" : model.state_dict(),  # model parameters\n",
    "    \"optimizer\" : optimizer.state_dict(),  # optimizer state\n",
    "    \"word2id\" : word2id, \n",
    "    \"id2word\" : id2word\n",
    "}\n",
    "torch.save(state, 'model-kn103.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = WordCompleter(list(word2embed.keys()))\n",
    "\n",
    "while True:\n",
    "    word = prompt(\"Word: \", completer = completer)\n",
    "    cc = int(input(\"Input closest count: \"))\n",
    "    try:\n",
    "        embed = word2embed[word]\n",
    "        print(embed, *getClose(embed, cc))\n",
    "    except KeyError:\n",
    "        print(\"Nonexistent word.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

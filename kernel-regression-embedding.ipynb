{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernelEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.embedding_weights = nn.Parameter(torch.randn(vocab_size, embedding_dim, dtype = torch.float64))\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, context, center):\n",
    "        context_vecs = torch.matmul(context, self.embedding_weights) # batch_size * (winlen - 1) * embedding\n",
    "        center_vec = torch.matmul(center, self.embedding_weights) # batch_size * embedding\n",
    "        diff = context_vecs - center_vec.unsqueeze(1)  # batch_size * (winlen - 1) * embedding\n",
    "        dist_sq = torch.sum(diff ** 2, dim=2)  # batch_size * (winlen - 1)\n",
    "        weights = torch.exp(-dist_sq / (2 * self.sigma ** 2))  # batch_size * (winlen - 1)\n",
    "        weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-8)  # batch_size * (winlen - 1)\n",
    "        weighted_context = (weights.unsqueeze(2) * context_vecs).sum(dim=1)  # batch_size * embedding\n",
    "\n",
    "        return weighted_context\n",
    "    \n",
    "    def getEmbedding(self, id):\n",
    "        return self.embedding_weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "    _ = re.findall(r\"[A-Za-z]+\", text)\n",
    "    words = []\n",
    "    for word in _:\n",
    "        words.append(word.lower())\n",
    "    word2id = {w : i for i, w in enumerate(set(words))}\n",
    "    id2word = {i : w for _, (w, i) in enumerate(word2id.items())}\n",
    "    return words, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen): # winlen must be odd\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    context_train = np.zeros((batch_size, winlen - 1, vocab_size))\n",
    "    center_train = np.zeros((batch_size, vocab_size))\n",
    "    for _ in range(winlen // 2, word_size - winlen // 2):\n",
    "        fr = _ - winlen // 2\n",
    "        center_train[fr][word2id[words[_]]] = 1\n",
    "        for __ in range(_ - winlen // 2, _):\n",
    "            context_train[fr][__ - (_ - winlen // 2)][word2id[words[__]]] = 1\n",
    "        for __ in range(_ + 1, _ + winlen // 2 + 1):\n",
    "            context_train[fr][__ - (_ - winlen // 2) - 1][word2id[words[__]]] = 1\n",
    "    return torch.tensor(context_train), torch.tensor(center_train), vocab_size, word_size\n",
    "        \n",
    "with open(\"input.txt\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, vocab_size, word_size = generateData(words, word2id, 5)\n",
    "print(context_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(context_train, center_train, embedding_dim):\n",
    "    model = GaussianKernelEmbedding(vocab_size, embedding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion1 = nn.CosineEmbeddingLoss()\n",
    "    num_epoches = 5000\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context_train, center_train)\n",
    "        target = torch.matmul(center_train, model.embedding_weights.detach())\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(context_train, center_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breeze 0 [-14.40703145   8.73204071  12.36670167 -16.03523951 -16.79188714]\n",
      "tall 1 [-16.97486237   3.38100845   3.27222886 -25.08567909  -8.4387105 ]\n",
      "sang 2 [ 0.33129743 -1.47820033  0.57870846  0.19120614 -1.19452783]\n",
      "vendors 3 [ 30.90371273 -38.63527094  26.80847221   0.91745414  10.19197026]\n",
      "fresh 4 [ 30.42413657 -34.08303314  30.05692052 -13.29498455  13.90778108]\n",
      "read 5 [ -8.39768939   8.73543871  17.02046513 -13.85261675 -11.54029867]\n",
      "in 6 [-17.10278034  20.0131912   22.2539325   12.13038196 -22.94085188]\n",
      "nearby 7 [ -1.56354254   2.1647209   18.64327164 -11.55797691  -4.48906593]\n",
      "gentle 8 [-16.59218113   6.76377744   9.06480991 -22.47058502 -18.42071867]\n",
      "park 9 [-12.98913973   5.82543449  10.50638427 -12.13077845 -14.73017129]\n",
      "birds 10 [ 0.33129743 -1.47820033  0.57870846  0.19120614 -1.19452783]\n",
      "warm 11 [ -1.56018485   2.16126245  18.63037589 -11.53853298  -4.48225302]\n",
      "chatted 12 [ 0.6929349   0.50316795  0.90907312 -2.09579454  0.72955842]\n",
      "shining 13 [-15.37864169  14.07971165  12.83476198  -5.93246407 -18.80915652]\n",
      "songs 14 [-15.0683163    1.79989271   8.52420297 -13.16825898 -17.69045318]\n",
      "cafe 15 [-11.45512735   2.63794353  12.22194307  -9.05129686  -7.91619207]\n",
      "enjoyed 16 [ 30.41889869 -34.06748836  30.06405033 -13.29494699  13.91140759]\n",
      "cars 17 [-7.16855878  8.11319302  8.52657048 -3.9774209  -8.61259131]\n",
      "perfect 18 [  6.17453728  -5.28667916  24.24540328 -19.22367143 -14.27398612]\n",
      "buzzed 19 [-12.63773964   5.97733903  10.29557777 -11.75854426 -14.42374214]\n",
      "selling 20 [ 28.46674034 -37.17510151  23.95829642  -0.32671775  14.79722308]\n",
      "dogs 21 [-7.16425495  8.11200377  8.52623142 -3.97508056 -8.59461851]\n",
      "customers 22 [-24.85740912  -4.10232979   3.52271069   0.71600679   2.09403431]\n",
      "various 23 [ 30.32461973 -40.53630833  24.91499186   5.37484339  11.37545303]\n",
      "leisurely 24 [ -7.19688977  21.39133035  22.48742982 -20.89671122 -16.68832528]\n",
      "happily 25 [-17.10376035  20.0197067   22.27089743  12.08274267 -22.97798536]\n",
      "walked 26 [-1.22762225  0.20629646 -2.11483027  0.89203999  0.68379571]\n",
      "life 27 [ -9.80434841   6.60270402   9.01203592  -8.11647541 -11.42090904]\n",
      "their 28 [-1.2276013   0.20629881 -2.11482565  0.89200997  0.68379627]\n",
      "relax 29 [ -0.88977058  -0.68534536  19.18684277 -15.30587119 -12.01836726]\n",
      "passing 30 [ 30.48994935 -32.96764557  31.63657214 -18.05030796  15.10746183]\n",
      "leaves 31 [-14.87491796   4.30490434   7.37866874 -16.07857953 -13.23132769]\n",
      "while 32 [-17.95233336   2.50862792   6.61519703 -22.54497946 -20.39230132]\n",
      "friends 33 [ 0.6929602   0.50317185  0.90910316 -2.09581732  0.72955905]\n",
      "children 34 [-16.23153024  18.53407984  20.05682972   7.57947236 -21.97580364]\n",
      "or 35 [ -8.72811488   4.2116206   14.50183982 -11.78929704 -10.93352838]\n",
      "was 36 [-14.53705378  10.13943617  10.91454374  -9.70949186 -16.61554347]\n",
      "rustled 37 [-15.03673797   8.71905971  11.54327214 -18.48269951 -17.12000142]\n",
      "coffee 38 [-17.84337558  -0.68693716   7.77213807  -3.93629356  -2.5410295 ]\n",
      "riverbank 39 [-2.69718581 -0.42735673 -0.186101    1.14411692  1.88519489]\n",
      "afternoon 40 [-11.91252284   5.45096659  11.64241948 -12.11695257 -13.8972964 ]\n",
      "street 41 [ 30.17647442 -36.59016391  26.93691453  -3.61725932  12.10925906]\n",
      "the 42 [-12.99383346   5.82522381  10.51141991 -12.13686848 -14.73748728]\n",
      "melodious 43 [-17.17722746   0.54222819   7.08303568 -17.43403806 -20.15749987]\n",
      "book 44 [ -8.12314157  13.13182348  19.3019595  -16.12658056 -12.58255159]\n",
      "to 45 [  2.57305096  -2.92573715  21.84758247 -17.36846984 -13.21844214]\n",
      "served 46 [-14.48911905   1.00608375   9.96310286  -6.4141307   -5.09764358]\n",
      "city 47 [ 13.28064931 -11.02711562  23.64057166 -21.8783233  -20.70349536]\n",
      "stroll 48 [ -5.97268015  18.71467385  18.9420065  -18.04644603 -14.74346946]\n",
      "brightly 49 [-15.46066444  13.79833314  12.44971179  -6.58134793 -18.61485279]\n",
      "a 50 [ -8.73728712   4.21220022  14.50748961 -11.81219807 -10.93572091]\n",
      "sitting 51 [  5.59470282  -5.2248103   24.0117385  -19.83691559 -15.96395129]\n",
      "along 52 [ -6.98709933  25.08187351  24.32271332 -23.33392094 -18.94099682]\n",
      "it 53 [-11.89629329   7.30659695  12.80340906 -10.83615046 -13.5212066 ]\n",
      "sun 54 [ 0.72009643 -1.47406123  1.46628575 -1.30285944 -0.94196507]\n",
      "as 55 [-15.23322524  15.58618855  15.29093787  -1.41103331 -19.83073152]\n",
      "small 56 [ -5.08423825   3.2420455   16.67652579 -11.76472353  -7.8316965 ]\n",
      "with 57 [-7.16648428  8.11278179  8.52635951 -3.97632152 -8.60625613]\n",
      "trees 58 [-17.85375742   2.96881925   1.54325644 -29.38460248  -5.61926596]\n",
      "people 59 [-18.61652375   2.49011627  -0.21040443 -33.44859073  -2.65436776]\n",
      "goods 60 [ 28.10203948 -39.36506105  22.42906995   4.41047201  15.33743293]\n",
      "and 61 [ 30.4514309  -34.09186322  30.07412656 -13.29659224  13.91277085]\n",
      "outside 62 [  9.41720674  -8.10691744  24.22402024 -21.14203911 -18.44994044]\n",
      "day 63 [ -4.68970208   1.65988815  16.72722443 -13.43086074 -11.1124728 ]\n",
      "played 64 [-15.56881672  17.1027049   17.76065885   3.03968726 -20.96722233]\n",
      "by 65 [ 30.31347217 -35.15729124  28.44085515  -8.48103401  12.80401924]\n",
      "of 66 [-16.14881689   3.78918836   5.18001783 -20.68390566 -11.16273752]\n",
      "pastries 67 [-21.45249933  -2.39384919   5.70563365  -1.62475477  -0.20829987]\n",
      "take 68 [ -7.67180884  17.38145076  21.06726596 -18.51388326 -14.35683822]\n"
     ]
    }
   ],
   "source": [
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach().numpy()\n",
    "    print(word, id, embedding)\n",
    "    word2embed[word] = embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernelEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.embedding_weights = nn.Parameter(torch.randn(vocab_size, embedding_dim, dtype = torch.float64))\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, context, center):\n",
    "        context_vecs = self.embedding_weights[context] # batch_size * (winlen - 1) * embedding\n",
    "        center_vec = self.embedding_weights[center] # batch_size * embedding\n",
    "        diff = context_vecs - center_vec.unsqueeze(1)  # batch_size * (winlen - 1) * embedding\n",
    "        dist_sq = torch.sum(diff ** 2, dim=2)  # batch_size * (winlen - 1)\n",
    "        weights = torch.exp(-dist_sq / (2 * self.sigma ** 2))  # batch_size * (winlen - 1)\n",
    "        weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-8)  # batch_size * (winlen - 1)\n",
    "        weighted_context = (weights.unsqueeze(2) * context_vecs).sum(dim=1)  # batch_size * embedding\n",
    "\n",
    "        return weighted_context\n",
    "    \n",
    "    def getEmbedding(self, id):\n",
    "        return self.embedding_weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1694556, 6]) torch.Size([1694556]) 27228\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text):\n",
    "    _ = re.findall(r\"[A-Za-z]+\", text)\n",
    "    words = []\n",
    "    for word in _:\n",
    "        words.append(word.lower())\n",
    "    word2id = {w : i for i, w in enumerate(set(words))}\n",
    "    id2word = {i : w for _, (w, i) in enumerate(word2id.items())}\n",
    "    return words, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen): # winlen must be odd\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    context_train = np.zeros((batch_size, winlen - 1))\n",
    "    center_train = np.zeros((batch_size))\n",
    "    for _ in range(winlen // 2, word_size - winlen // 2):\n",
    "        fr = _ - winlen // 2\n",
    "        center_train[fr] = word2id[words[_]]\n",
    "        for __ in range(_ - winlen // 2, _):\n",
    "            context_train[fr][__ - (_ - winlen // 2)] = word2id[words[__]]\n",
    "        for __ in range(_ + 1, _ + winlen // 2 + 1):\n",
    "            context_train[fr][__ - (_ - winlen // 2) - 1] = word2id[words[__]]\n",
    "    return torch.tensor(context_train).int(), torch.tensor(center_train).int(), vocab_size, word_size\n",
    "        \n",
    "with open(\"wiki.train.tokens\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, vocab_size, word_size = generateData(words, word2id, 5)\n",
    "print(context_train.shape, center_train.shape, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, context_train, center_train):\n",
    "    criterion = nn.CosineEmbeddingLoss()\n",
    "    flag = torch.ones(context_train.shape[0])\n",
    "    num_epoches = 1000\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context_train, center_train)\n",
    "        target = model.embedding_weights[center_train].detach()\n",
    "        loss = criterion(output, target, flag)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "        elif epoch > 100 and loss.item() > best_loss * 1.05: \n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GaussianKernelEmbedding(vocab_size, 32)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = torch.load(\"model-32.pth\")\n",
    "model.load_state_dict(current_model[\"state_dict\"])\n",
    "optimizer.load_state_dict(current_model[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = train(model, optimizer, context_train, center_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach()\n",
    "    word2embed[word] = embedding\n",
    "\n",
    "with open(\"output.txt\", 'w') as f:\n",
    "    for (word, embed) in word2embed.items():\n",
    "        f.write(word)\n",
    "        f.write(str(list(embed.numpy())))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClose(target):\n",
    "    sims = []\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    for (word, embed) in word2embed.items():\n",
    "        sim = cos(embed, target).item()\n",
    "        sims.append((word, sim))\n",
    "\n",
    "    res = sorted(sims, key=lambda x : x[1], reverse = True)\n",
    "    return res[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athens greece baghdad cassette\n",
      "athens greece bangkok smart\n",
      "athens greece beijing coatings\n",
      "athens greece berlin greece\n",
      "athens greece bern ambitious\n",
      "athens greece cairo isolated\n",
      "athens greece canberra humanitarian\n",
      "athens greece hanoi greece\n",
      "athens greece havana doom\n",
      "athens greece helsinki beno\n",
      "athens greece london smart\n",
      "athens greece madrid pbs\n",
      "athens greece moscow pentwyn\n",
      "athens greece oslo incoherent\n",
      "athens greece ottawa isolated\n",
      "athens greece paris greece\n",
      "athens greece rome greece\n",
      "athens greece stockholm greece\n",
      "athens greece tehran incoherent\n",
      "athens greece tokyo phosphorus\n",
      "baghdad iraq bangkok exercises\n",
      "baghdad iraq beijing stunts\n",
      "baghdad iraq berlin narragansett\n",
      "baghdad iraq bern stunts\n",
      "baghdad iraq cairo reforming\n",
      "baghdad iraq canberra baghdad\n",
      "baghdad iraq hanoi dollar\n",
      "baghdad iraq havana baghdad\n",
      "baghdad iraq helsinki possessed\n",
      "baghdad iraq london swears\n",
      "baghdad iraq madrid baghdad\n",
      "baghdad iraq moscow stunts\n",
      "baghdad iraq oslo stunts\n",
      "baghdad iraq ottawa narragansett\n",
      "baghdad iraq paris stunts\n",
      "baghdad iraq rome extraterrestrial\n",
      "baghdad iraq stockholm stunts\n",
      "baghdad iraq tehran baghdad\n",
      "baghdad iraq tokyo baghdad\n",
      "baghdad iraq athens extraterrestrial\n",
      "bangkok thailand beijing ghora\n",
      "bangkok thailand berlin jewels\n",
      "bangkok thailand bern arikaree\n",
      "bangkok thailand cairo affirmation\n",
      "bangkok thailand canberra traditionally\n",
      "bangkok thailand hanoi jewels\n",
      "bangkok thailand havana specialised\n",
      "bangkok thailand helsinki dreamworld\n",
      "bangkok thailand london moreau\n",
      "bangkok thailand madrid bangkok\n",
      "bangkok thailand moscow bangkok\n",
      "bangkok thailand oslo bangkok\n",
      "bangkok thailand ottawa affirmation\n",
      "bangkok thailand paris harlem\n",
      "bangkok thailand rome jewels\n",
      "bangkok thailand stockholm bangkok\n",
      "bangkok thailand tehran bangkok\n",
      "bangkok thailand tokyo bangkok\n",
      "bangkok thailand athens jewels\n",
      "bangkok thailand baghdad egyptians\n",
      "beijing china berlin moresby\n",
      "beijing china bern rosey\n",
      "beijing china cairo recess\n",
      "beijing china canberra pareiasaur\n",
      "beijing china hanoi showbiz\n",
      "beijing china havana grossman\n",
      "beijing china helsinki andrzej\n",
      "beijing china london andrzej\n",
      "beijing china madrid andrzej\n",
      "beijing china moscow delayed\n",
      "beijing china oslo enzyme\n",
      "beijing china ottawa andrzej\n",
      "beijing china paris imaginary\n",
      "beijing china rome chronological\n",
      "beijing china stockholm berkley\n",
      "beijing china tehran delayed\n",
      "beijing china tokyo avenge\n",
      "beijing china athens cross\n",
      "beijing china baghdad flagship\n",
      "beijing china bangkok burrows\n",
      "berlin germany bern paralleled\n",
      "berlin germany cairo berlin\n",
      "berlin germany canberra conflict\n",
      "berlin germany hanoi teng\n",
      "berlin germany havana indifferent\n",
      "berlin germany helsinki dysfunctional\n",
      "berlin germany london revised\n",
      "berlin germany madrid proposals\n",
      "berlin germany moscow engines\n",
      "berlin germany oslo palisade\n",
      "0 90\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "correctCount = 0\n",
    "totalCount = 0\n",
    "with open(\"questions-words.txt\", 'r') as f:\n",
    "    qs = f.read()\n",
    "\n",
    "qs_s = qs.split('\\n')\n",
    "#random.shuffle(qs_s)\n",
    "\n",
    "for q in qs_s[:100]:\n",
    "    words = q.split()\n",
    "    try:\n",
    "        ans = getClose(word2embed[words[0].lower()] + word2embed[words[1].lower()] - word2embed[words[2].lower()])[0][0]\n",
    "        if ans == words[3].lower():\n",
    "            correctCount += 1\n",
    "        else:\n",
    "            print(words[0].lower(), words[1].lower(), words[2].lower(), ans)\n",
    "        totalCount += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(correctCount, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'state_dict': model.state_dict(),  # model parameters\n",
    "    'optimizer': optimizer.state_dict(),  # optimizer state\n",
    "}\n",
    "torch.save(state, 'model-64.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

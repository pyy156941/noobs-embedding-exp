{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from prompt_toolkit import prompt\n",
    "from prompt_toolkit.completion import WordCompleter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbowEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, winlen):\n",
    "        super().__init__()\n",
    "        self.embedding_weights = nn.Parameter(torch.randn(vocab_size, embedding_dim, dtype = torch.float32, device = device) * (1.0 / embedding_dim ** 0.5))\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_vecs = self.embedding_weights[context] # batch_size * (winlen - 1) * embedding\n",
    "        avg_vecs = context_vecs.mean(dim = 1)\n",
    "        output = self.fc(avg_vecs)\n",
    "        return output\n",
    "    \n",
    "    def getEmbedding(self, id):\n",
    "        return self.embedding_weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1559034, 2]) torch.Size([1559034]) 7715\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text, min_count = 20, threshold = 1e-3):\n",
    "    _ = re.findall(r\"[A-Za-z]+\", text)\n",
    "    words = []\n",
    "    for word in _:\n",
    "        words.append(word.lower())\n",
    "    word_counts = Counter(words)\n",
    "    words = [word for word in words if word_counts[word] >= min_count]\n",
    "\n",
    "    sub_words = []\n",
    "    for word in words:\n",
    "        freq = word_counts[word] / len(words)\n",
    "        discard_prob = 1.0 - np.sqrt(threshold / freq)\n",
    "\n",
    "        if np.random.rand() > discard_prob:\n",
    "            sub_words.append(word)\n",
    "\n",
    "    word2id = {w : i for i, w in enumerate(set(sub_words))}\n",
    "    id2word = {i : w for _, (w, i) in enumerate(word2id.items())}\n",
    "    return words, word2id, id2word\n",
    "\n",
    "def generateData(words, word2id, winlen): # winlen must be odd\n",
    "    vocab_size = len(word2id)\n",
    "    word_size = len(words)\n",
    "    batch_size = word_size - winlen + 1\n",
    "    context_train = np.zeros((batch_size, winlen - 1))\n",
    "    center_train = np.zeros((batch_size))\n",
    "    for _ in range(winlen // 2, word_size - winlen // 2):\n",
    "        fr = _ - winlen // 2\n",
    "        center_train[fr] = word2id[words[_]]\n",
    "        for __ in range(_ - winlen // 2, _):\n",
    "            context_train[fr][__ - (_ - winlen // 2)] = word2id[words[__]]\n",
    "        for __ in range(_ + 1, _ + winlen // 2 + 1):\n",
    "            context_train[fr][__ - (_ - winlen // 2) - 1] = word2id[words[__]]\n",
    "    return torch.tensor(context_train).int(), torch.tensor(center_train).int(), vocab_size, word_size\n",
    "        \n",
    "with open(\"wiki-2.train.tokens\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words, word2id, id2word = preprocessing(text)\n",
    "context_train, center_train, vocab_size, word_size = generateData(words, word2id, 3)\n",
    "print(context_train.shape, center_train.shape, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, context_train, center_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    num_epoches = 1000\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_context, batch_center in tqdm(dataloader):\n",
    "            # Move data to device\n",
    "            batch_context = batch_context.to(device)\n",
    "            batch_center = batch_center.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_context)\n",
    "            target = batch_center.long()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * len(batch_context)\n",
    "        \n",
    "        epoch_loss /= len(context_train)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "        elif epoch > 20 and loss > best_loss * 1.05: \n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = CbowEmbedding(vocab_size, 80, 3).to(device)\n",
    "model.embedding_weights.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = torch.load(\"model-2.pth\", map_location=device)\n",
    "model.load_state_dict(current_model[\"state_dict\"])\n",
    "optimizer.load_state_dict(current_model[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset = CorpusDataset(context_train, center_train), \n",
    "    batch_size = 256, \n",
    "    shuffle = True, \n",
    "    num_workers = 4,\n",
    "    pin_memory = True if torch.cuda.is_available() else False\n",
    ")\n",
    "print(len(dataloader))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} | Device: {param.device}\")\n",
    "model, optimizer = train(model, optimizer, context_train, center_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embed = {}\n",
    "for (word, id) in word2id.items():\n",
    "    embedding = model.getEmbedding(id).detach().to(\"cpu\")\n",
    "    embedding = embedding / torch.norm(embedding)\n",
    "    word2embed[word] = embedding\n",
    "\n",
    "with open(\"output-cbow.txt\", 'w') as f:\n",
    "    for (word, embed) in word2embed.items():\n",
    "        f.write(word)\n",
    "        f.write(str(list(embed.numpy())))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClose(target, k = 5):\n",
    "    sims = []\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    for (word, embed) in word2embed.items():\n",
    "        sim = cos(embed, target).item()\n",
    "        sims.append((word, sim))\n",
    "\n",
    "    res = sorted(sims, key=lambda x : x[1], reverse = True)\n",
    "    return [_[0] for _ in res[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poland polish chile ['chile', 'polish', 'foxes', 'sierra', 'prohaska', 'alpha', 'platte', 'frost', 'sammy', 'imported']\n",
      "work works predict ['works', 'predict', 'darling', 'winslow', 'attachment', 'tanaka', 'charting', 'naked', 'inhabited', 'nucleus']\n",
      "germany german ukraine ['german', 'ukraine', 'atlanta', 'motives', 'researched', 'poehler', 'social', 'forwards', 'tribal', 'atop']\n",
      "high highest strong ['highest', 'strong', 'wales', 'distributor', 'solitude', 'personality', 'insert', 'ganymede', 'denmark', 'market']\n",
      "goat goats machine ['goats', 'machine', 'manitoba', 'foca', 'mold', 'demonstrations', 'spoken', 'valuable', 'scouting', 'r']\n",
      "short shortest great ['shortest', 'great', 'desirable', 'method', 'mitchells', 'prospect', 'testament', 'undertake', 'engineered', 'befriended']\n",
      "decreasing decreased going ['going', 'decreased', 'ruined', 'composite', 'about', 'annexed', 'cape', 'with', 'wife', 'cleared']\n",
      "mexico mexican germany ['germany', 'mexican', 'thoroughly', 'wine', 'with', 'identified', 'terraces', 'tide', 'resupply', 'sol']\n",
      "lion lions eye ['lions', 'eye', 'clearly', 'resistant', 'thai', 'keamy', 'enabling', 'weaving', 'panther', 'subsidiary']\n",
      "strong strongest weak ['strongest', 'weak', 'transept', 'tennis', 'usher', 'ensuing', 'injuries', 'ancestor', 'opens', 'sur']\n",
      "mexico mexican sweden ['sweden', 'mexican', 'peaks', 'signals', 'tharp', 'appropriate', 'atp', 'memoirs', 'sr', 'organizers']\n",
      "cat cats finger ['finger', 'cats', 'des', 'back', 'school', 'berman', 'tasks', 'opened', 'maclagen', 'uncovered']\n",
      "goat goats computer ['goats', 'computer', 'valuable', 'slip', 'hansen', 'tenor', 'refers', 'muscular', 'wolverhampton', 'houllier']\n",
      "aware unaware productive ['productive', 'unaware', 'alkan', 'canons', 'radial', 'cite', 'jurchen', 'cream', 'rpg', 'created']\n",
      "moving moved flying ['flying', 'moved', 'considering', 'brutal', 'gabrielle', 'theorist', 'logical', 'renamed', 'lewis', 'impressive']\n",
      "hitting hit sitting ['sitting', 'hit', 'chain', 'bessin', 'designing', 'june', 'tower', 'worst', 'convince', 'whereas']\n",
      "walk walks generate ['generate', 'walks', 'updated', 'o', 'guild', 'train', 'toned', 'quartet', 'mentally', 'molecular']\n",
      "typical typically swift ['typically', 'swift', 'convex', 'manhattan', 'plumes', 'outright', 'wright', 'fleming', 'moth', 'evacuated']\n",
      "germany german poland ['german', 'poland', 'charting', 'motives', 'forwards', 'reformed', 'busy', 'cheese', 'correct', 'repairs']\n",
      "strong strongest cold ['strongest', 'cold', 'universal', 'transept', 'slender', 'columnist', 'harmonix', 'heroin', 'since', 'emplacements']\n",
      "consistent inconsistent acceptable ['acceptable', 'inconsistent', 'influence', 'inauguration', 'prisoner', 'progress', 'relevant', 'condemned', 'charged', 'honor']\n",
      "usa dollar romania ['romania', 'dollar', 'churchill', 'artillery', 'follows', 'intellectuals', 'marvel', 'concluded', 'length', 'genuine']\n",
      "decreasing decreased sleeping ['decreased', 'sleeping', 'emerging', 'initiative', 'cape', 'walter', 'christopher', 'admiration', 'craig', 'extras']\n",
      "sudden suddenly possible ['possible', 'suddenly', 'del', 'reef', 'specimen', 'justify', 'pagodas', 'grass', 'contact', 'picture']\n",
      "new newer high ['newer', 'high', 'perception', 'congress', 'brethren', 'physicist', 'sicily', 'urging', 'krak', 'guitarist']\n",
      "switzerland swiss japan ['japan', 'swiss', 'issues', 'precise', 'musically', 'criminals', 'dalmeny', 'insists', 'container', 'ming']\n",
      "0 26\n"
     ]
    }
   ],
   "source": [
    " import random\n",
    "\n",
    "correctCount = 0\n",
    "totalCount = 0\n",
    "with open(\"questions-words.txt\", 'r') as f:\n",
    "    qs = f.read()\n",
    "\n",
    "qs_s = qs.split('\\n')\n",
    "random.shuffle(qs_s)\n",
    "\n",
    "for q in qs_s[:100]:\n",
    "    words = q.split()\n",
    "    try:\n",
    "        target = word2embed[words[2].lower()] + word2embed[words[1].lower()] - word2embed[words[0].lower()]\n",
    "        target = target / torch.norm(target)\n",
    "        ans = getClose(target, 10)\n",
    "        if words[3].lower() in ans:\n",
    "            correctCount += 1\n",
    "        print(words[0].lower(), words[1].lower(), words[2].lower(), ans)\n",
    "        totalCount += 1\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(correctCount, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'state_dict': model.state_dict(),  # model parameters\n",
    "    'optimizer': optimizer.state_dict(),  # optimizer state\n",
    "}\n",
    "torch.save(state, 'model-2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Input is not a terminal (fd=0).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m completer \u001b[38;5;241m=\u001b[39m WordCompleter(\u001b[38;5;28mlist\u001b[39m(word2embed\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWord: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompleter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompleter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         embed \u001b[38;5;241m=\u001b[39m word2embed[word]\n",
      "File \u001b[0;32m~/.globalenv/lib/python3.12/site-packages/prompt_toolkit/shortcuts/prompt.py:1425\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(message, history, editing_mode, refresh_interval, vi_mode, lexer, completer, complete_in_thread, is_password, key_bindings, bottom_toolbar, style, color_depth, cursor, include_default_pygments_style, style_transformation, swap_light_and_dark_colors, rprompt, multiline, prompt_continuation, wrap_lines, enable_history_search, search_ignore_case, complete_while_typing, validate_while_typing, complete_style, auto_suggest, validator, clipboard, mouse_support, input_processors, placeholder, reserve_space_for_menu, enable_system_prompt, enable_suspend, enable_open_in_editor, tempfile_suffix, tempfile, default, accept_default, pre_run, set_exception_handler, handle_sigint, in_thread, inputhook)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# The history is the only attribute that has to be passed to the\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# `PromptSession`, it can't be passed into the `prompt()` method.\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m session: PromptSession[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m PromptSession(history\u001b[38;5;241m=\u001b[39mhistory)\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mediting_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mediting_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefresh_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefresh_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvi_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvi_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompleter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompleter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplete_in_thread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplete_in_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_bindings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_bindings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbottom_toolbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom_toolbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_default_pygments_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_default_pygments_style\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstyle_transformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle_transformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mswap_light_and_dark_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_light_and_dark_colors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_continuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_continuation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrap_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_history_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_history_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_ignore_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_ignore_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplete_while_typing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplete_while_typing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_while_typing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_while_typing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplete_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplete_style\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_suggest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_suggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclipboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmouse_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmouse_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_processors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_processors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplaceholder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserve_space_for_menu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserve_space_for_menu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_suspend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_suspend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_open_in_editor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_open_in_editor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtempfile_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtempfile_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtempfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_default\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_default\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_exception_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_exception_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_sigint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle_sigint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_thread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.globalenv/lib/python3.12/site-packages/prompt_toolkit/shortcuts/prompt.py:1035\u001b[0m, in \u001b[0;36mPromptSession.prompt\u001b[0;34m(self, message, editing_mode, refresh_interval, vi_mode, lexer, completer, complete_in_thread, is_password, key_bindings, bottom_toolbar, style, color_depth, cursor, include_default_pygments_style, style_transformation, swap_light_and_dark_colors, rprompt, multiline, prompt_continuation, wrap_lines, enable_history_search, search_ignore_case, complete_while_typing, validate_while_typing, complete_style, auto_suggest, validator, clipboard, mouse_support, input_processors, placeholder, reserve_space_for_menu, enable_system_prompt, enable_suspend, enable_open_in_editor, tempfile_suffix, tempfile, default, accept_default, pre_run, set_exception_handler, handle_sigint, in_thread, inputhook)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dumb_prompt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mas\u001b[39;00m dump_app:\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dump_app\u001b[38;5;241m.\u001b[39mrun(in_thread\u001b[38;5;241m=\u001b[39min_thread, handle_sigint\u001b[38;5;241m=\u001b[39mhandle_sigint)\n\u001b[0;32m-> 1035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_exception_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_exception_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_thread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_sigint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle_sigint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.globalenv/lib/python3.12/site-packages/prompt_toolkit/application/application.py:1002\u001b[0m, in \u001b[0;36mApplication.run\u001b[0;34m(self, pre_run, set_exception_handler, handle_sigint, in_thread, inputhook)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(coro)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;66;03m# No loop installed. Run like usual.\u001b[39;00m\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "completer = WordCompleter(list(word2embed.keys()))\n",
    "\n",
    "while True:\n",
    "    word = prompt(\"Word: \", completer = completer)\n",
    "    try:\n",
    "        embed = word2embed[word]\n",
    "        print(embed, getClose(embed))\n",
    "    except KeyError:\n",
    "        print(\"Nonexistent word.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
